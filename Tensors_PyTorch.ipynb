{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKSyd5je+Lvggy0wkQzrj1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepaksirohiwal/Deep_Learning_with_PyTorch/blob/main/Tensors_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jJZ-zeDLp2Ze"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are Tensors?\n",
        "\n",
        "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.\n",
        "\n",
        "Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data"
      ],
      "metadata": {
        "id": "B7kWzbcaqeWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a 2x3 matrix (2 rows, 3 columns)\n",
        "data = [[1, 2, 3], [4, 5, 6]]\n",
        "tensor = torch.tensor(data)\n",
        "\n",
        "print(tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhZZKxDhp6B_",
        "outputId": "43d9344b-d8c8-458c-dff2-892be3ad132d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensors can be created from numpy array\n",
        "np_array= np.array(data)\n",
        "tensor_np=torch.from_numpy(np_array)\n",
        "print(tensor_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjt-uCfyqGuo",
        "outputId": "dcb37476-f072-44ce-81c0-517a8115a745"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new tensor retains the properties of the argument tensor unless explicitly overridden\n",
        "x_ones=torch.ones_like(tensor)\n",
        "print(f\"Ones tensors: \\n {x_ones}\\n\")\n",
        "\n",
        "x_rand= torch.rand_like(tensor, dtype=torch.float)\n",
        "print(f\"Random Tensor: \\n {x_rand}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2pay0ZbuFtd",
        "outputId": "dfdb1817-d466-4454-8a76-98176253d0d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones tensors: \n",
            " tensor([[1, 1, 1],\n",
            "        [1, 1, 1]])\n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.5507, 0.9277, 0.7524],\n",
            "        [0.8279, 0.8253, 0.4156]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor with radom or contant values\n",
        "shape=(2,3)\n",
        "rand_tensor=torch.rand(shape)\n",
        "ones_tensor=torch.ones(shape)\n",
        "zeros_tensor=torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor} \\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFBRYGi9wrC_",
        "outputId": "564ccd71-9035-449a-b7b3-30079caddff1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.2086, 0.2974, 0.5643],\n",
            "        [0.2029, 0.6316, 0.7974]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# attributes of tensors\n",
        "tensor_x= torch.rand(3,4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor_x.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor_x.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor_x.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvaG1YnLxmAF",
        "outputId": "481f32c4-436a-4148-bc3a-56aa149d04cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of these operations can be run on the GPU (at typically higher speeds than on a CPU). If you’re using Colab, allocate a GPU by going to Runtime > Change runtime type > GPU.\n",
        "\n",
        "By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability)"
      ],
      "metadata": {
        "id": "2QOFetsC2sAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  tensor=tensor.to(\"cuda\")"
      ],
      "metadata": {
        "id": "zvwj2udW2LEM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sclicing\n",
        "tensor_x= torch.rand(4,4)\n",
        "print(tensor_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te1jtlFe22-f",
        "outputId": "d8b68b2d-142f-49b9-d703-527b39fd44f8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9635, 0.8880, 0.9644, 0.8368],\n",
            "        [0.9456, 0.0074, 0.1670, 0.5148],\n",
            "        [0.6015, 0.9124, 0.0446, 0.8916],\n",
            "        [0.1510, 0.8542, 0.8114, 0.4277]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First row: {tensor_x[0]}\")\n",
        "print(f\"First column: {tensor_x[:,0]}\")\n",
        "print(f\"Last column: {tensor_x[...,-1]}\")\n",
        "tensor_x[:,1]=0\n",
        "print(tensor_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmzbLP1W5c3K",
        "outputId": "38e4895f-ffaf-4ccb-8fcd-5a7ea5185632"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First row: tensor([0.9635, 0.8880, 0.9644, 0.8368])\n",
            "First column: tensor([0.9635, 0.9456, 0.6015, 0.1510])\n",
            "Last column: tensor([0.8368, 0.5148, 0.8916, 0.4277])\n",
            "tensor([[0.9635, 0.0000, 0.9644, 0.8368],\n",
            "        [0.9456, 0.0000, 0.1670, 0.5148],\n",
            "        [0.6015, 0.0000, 0.0446, 0.8916],\n",
            "        [0.1510, 0.0000, 0.8114, 0.4277]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arithmetic operations"
      ],
      "metadata": {
        "id": "Qqb4i_bBBYRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor_y= torch.ones(4,4)\n",
        "if torch.cuda.is_available():\n",
        "  tensor_y=tensor_y.to(\"cuda\")\n",
        "# Arithmetic operations\n",
        "# tensor_y.T returns the transpose of the tensor\n",
        "y1= tensor_y @ tensor_y.T\n",
        "y2= tensor_y.matmul(tensor_y.T)\n",
        "\n",
        "print(f\"y1: {y1}\")\n",
        "print(f\"y2: {y2}\")\n",
        "\n",
        "# element wise product\n",
        "z1=tensor_y*tensor_y\n",
        "z2=tensor_y.mul(tensor_y)\n",
        "\n",
        "print(f\"z1: {z1}\")\n",
        "print(f\"z2: {z2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1pT_HUG55Ds",
        "outputId": "122e2f53-fcad-4c2d-89ee-87aba59d72a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y1: tensor([[4., 4., 4., 4.],\n",
            "        [4., 4., 4., 4.],\n",
            "        [4., 4., 4., 4.],\n",
            "        [4., 4., 4., 4.]])\n",
            "y2: tensor([[4., 4., 4., 4.],\n",
            "        [4., 4., 4., 4.],\n",
            "        [4., 4., 4., 4.],\n",
            "        [4., 4., 4., 4.]])\n",
            "z1: tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "z2: tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single-element tensors If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using item()"
      ],
      "metadata": {
        "id": "ybSHeFheGD1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# single element tensors\n",
        "agg= tensor_y.sum()\n",
        "agg_item= agg.item()\n",
        "print(agg_item, type(agg_item))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxnSc2Wd9tyr",
        "outputId": "6e0d3884-cfd3-4b78-f2ef-272cbde0765d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.0 <class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inplace operation\n",
        "Operations that store the result into the operand are called in-place. They are denoted by a `_` suffix. For example: `x.copy_(y)`, `x.t_()`, will change x."
      ],
      "metadata": {
        "id": "QHuuBwkPwOex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{tensor_y}\\n\")\n",
        "tensor_y.add_(5)\n",
        "print(tensor_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "susWZjWwvw3v",
        "outputId": "4b642933-84ba-4f6c-c844-215dca4de2a9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "\n",
            "tensor([[6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.],\n",
            "        [6., 6., 6., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bridge with numpy\n",
        "Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.\n",
        "\n",
        "\n",
        "---\n",
        "You can convert data between PyTorch tensors and NumPy arrays without necessarily copying the data. This sharing of memory is possible due to the design and interoperability of PyTorch and NumPy, making it efficient to switch between the two libraries.\n",
        "\n",
        "Here's how this works:\n",
        "\n",
        "1. **Shared Memory:** Both PyTorch and NumPy offer a way to work with numerical data in a way that is compatible with each other. When you create a PyTorch tensor from a NumPy array or vice versa, they can potentially share the same memory location.\n",
        "\n",
        "2. **Efficient Conversion:** When you convert data between a PyTorch tensor and a NumPy array, the libraries often do not perform a full data copy. Instead, they create a view that points to the same memory location.\n",
        "\n",
        "3. **In-Place Modifications:** Since they share memory, if you modify the values in one (either through PyTorch or NumPy operations), the other will immediately reflect those changes. This can be very useful for interoperability and memory efficiency.\n",
        "\n",
        "However, it's important to note that not all conversions between PyTorch and NumPy result in shared memory. Sometimes, a conversion might require a full copy of the data if the underlying data type or layout is incompatible. Also, sharing memory between the two libraries can be risky if you're not aware of the potential side effects.\n",
        "\n",
        "To ensure you understand how memory sharing works and to avoid unexpected behavior, it's recommended to handle conversions deliberately and carefully, especially in complex scenarios where you're dealing with large datasets or complex computations.\n"
      ],
      "metadata": {
        "id": "uGzi4d-RwusC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a NumPy array\n",
        "numpy_array = np.array([1, 2, 3])\n",
        "\n",
        "# Convert NumPy array to PyTorch tensor\n",
        "pytorch_tensor = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Modify the tensor in-place\n",
        "pytorch_tensor[0] = 99\n",
        "\n",
        "# The corresponding element in the NumPy array also changes\n",
        "print(numpy_array)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67v7qrZewirk",
        "outputId": "a9b90478-2286-4951-bae4-69d075478b44"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[99  2  3]\n"
          ]
        }
      ]
    }
  ]
}